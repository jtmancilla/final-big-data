---
title: "Reporte"
author: "Equipo TienenDiez"
date: "3 de junio de 2015"
output: html_document
---

## Introducción

La demanda de procesamiento de flujos en tiempo real está aumentando cada vez más. La razón es que a menudo el procesamiento de grandes volúmenes de datos no es suficiente. Los datos tiene que ser procesados rápidos para que una empresa pueda reaccionar a las condiciones cambiantes del negocio en tiempo real.

Web logs, RFID, sensor networks, social networks, social data, páginas de Internet, Indexado y búsqueda de páginas de internet, detalle de llamadas, datos astronómicos, ciencia, datos genómicos, biogeoquímicos, biológicos, vigilancia (cámas de vídeo, por ejemplo), biogeoquímicos, biológicos, registros médicos, fotografías, vídeo, transacciones bancarias, son ejemplos de lo que se puede procesar.

¿Qué es Big data?

Es un concepto relativo (como todo aquello que está relacionado con el tamaño)
Cuando la información no viene en formatos estructurados
Crecimiento acelerado en la adquisición de datos, hardware, alamacenamiento, paralelismo, tiempo de proceso, etc.
Análisis de datos de varias fuentes distintas

<br></br>

## Datos GDELT

GDELT es un proyecto que contiene más de 300 millones de eventos geolocalizados desde 1979 a la fecha. Los datos se basan en informes de noticias de una variedad de fuentes internacionales codificadas usando el sistema de TABARI para registrar los eventos y el uso de software adicional para la ubicación y el tono. Los datos son de libre disposición y actualizadas diariamente.

<br></br>

## Uso potencial de los datos GDELT

La base de GDELT nos puede ofrecer registros de eventos de conflictos a lo largo de toda la república mexicana y del mundo. Como por ejemplo podemos distinguir en el siguiente gráfico las zonas con mayores conflictos registrados.

- la zona fronteriza del norte (Tijuana y Ciudad Juarez)
- la zona centro (Estado de México y Guerrero)
- la zona sur (Chiapas y Oaxaca)

<center><img  src="/Users/usuario/Dropbox/proyecto_final/images/m2.png" height="800px" width="620px" /></center>

<br></br>

## Data Flow

- Recolección de documentos GDELT de la página con Flume
- Importación de documentos de carpetas hacia HDFS - AVRO
- Uso Hive, Pig y Spark para procesar los datos
- Importar los datos procesados a R
- Visualización

------------
GDELT
------------

* Obtención
    + Diariamente, scrappear la página de GDELT para ver si han subido algo nuevo.
    + Cuando suban algo nuevo, bajar las bases nuevas (HTML).
* Limpieza
    + Generar el CSV a partir del HTML.
    + Limpiar y validar el CSV (bash).
* Carga
    + Postgres
	- Subir los datos nuevos y actualizar las estadísticas.
	- Actualizar los índices.
    + Hadoop
	- Subir al HDFS y agregarlo al dataset en formato Avro.
* Manipulación
    + Actualizar bases resumidas para el Shiny (series de tiempo, etc) dentro de Postgres.
    + Agregar los nuevos datos al Parquet para analítica dentro del HDFS.


<br></br>

## Tareas

- Bajar datos periodicamente.
- Usar un spoolDir de Flume para subirlo a Hive.
- Subirlo como Avro directo al HDFS.
- Analítica en Hive
- Transformar la base cruda en Pig o Spark a una base para análisis.
- Orquestación con Luigi

<br></br>

## Ejecución

```{r, eval=FALSE }

### Desde playground/felipe (en compu local o en docker)

# Generar datasets dummies para los mayores a 20 MB para pruebas
./generate_dummy_csv data/raw 20000000

# Crear la instancia
docker run -ti --name hadoop-pseudo-proyecto2 \
	   -v /Users/Felipe/data-science/final-big-data:/home/itam/localhost \
	   -p 2122:2122 -p 2181:2181 -p 39534:39534 -p 9000:9000 \
	   -p 50070:50070 -p 50010:50010 -p 50020:50020 -p 50075:50075 \
	   -p 50090:50090 -p 8030:8030 -p 8031:8031 -p 8032:8032 \
	   -p 8033:8033 -p 8088:8088 -p 8040:8040 -p 8042:8042 \
	   -p 13562:13562 -p 47784:47784 -p 10020:10020 -p 19888:19888 \
	   -p 8000:8000 -p 9999:9999 \
	   --net=host \
	   nanounanue/docker-hadoop
# ISSUE [SOLVED]: En algunas redes se necesita el --net=host

# Cambiar al usuario itam
su itam

# Para tener el comando man
sudo apt-get install man-db

# Necesario para usar Flume
sudo ln -s /usr/lib/kite/kite-morphlines-* /usr/lib/flume-ng/lib
sudo ln -s /usr/lib/kite/lib/metrics* /usr/lib/flume-ng/lib
sudo ln -s /usr/lib/kite/lib/config-1.0.2.jar /usr/lib/flume-ng/lib
sudo ln -s /usr/lib/kite/lib/Saxon-HE-9.5.1-5.jar /usr/lib/flume-ng/lib
sudo ln -s /usr/lib/kite/lib/tika-* /usr/lib/flume-ng/lib

# Correr el contenedor si ya se había creado
docker start -ia hadoop-pseudo-proyecto

#---------------------------------------------------------------------------------------------------------------
# Crear las carpetas de prueba. Cuando esto quede, habrá que hacerlo en /etl etc, en lugar de /user/itam...
sudo -u hdfs hadoop fs -mkdir /user/itam
sudo -u hdfs hadoop fs -chown itam:itam /user/itam

#---------------------------------------------------------------------------------------------------------------
# Generar el esquema de los datos (desde playground/felipe)
curl http://gdeltproject.org/data/lookups/CSV.header.dailyupdates.txt > data/gdelt_headers.tsv
unzip -p data/raw/20130412.export.CSV.zip | cat data/gdelt_headers.tsv - > data/schema/20130412.export.CSV
kite-dataset csv-schema data/schema/20130412.export.CSV --class GDELT --delimiter "\t" -o data/schema/gdelt_raw20130412.avsc
# IMPORTANTE: Hay que cambiar el tipo de Actor1Geo_FeatureID y Actor2Geo_FeatureID a string manualmente y renombrarlo a gdelt.avsc.
# ISSUE: POR QUE AL LEER EL CSV DE 20130409 LEE PUROS NULLS??
# [SOLVED] Tiene que tener los headers para que lo lea! Para leer con Flume no hacen falta, así que no los ponemos después.

# Crear el dataset en el hive metastore (desde playground/felipe)
kite-dataset create dataset:hive:gdelt --schema data/schema/gdelt.avsc 

# El dataset queda en: /user/hive/warehouse/gdelt
hadoop fs -ls -R /user/hive

#---------------------------------------------------------------------------------------------------------------
### BEGIN NOT RUN ###
# Probamos importar unos datos
#unzip -p data/raw/20130412.export.CSV.zip | cat data/gdelt_headers.tsv - > data/spool/20130412.export.CSV
#kite-dataset csv-import data/spool/20130412.export.CSV dataset:hive:gdelt --delimiter "\t"
#hadoop fs -ls -R /user/hive

# Podemos ver los datos en HIVE
#beeline -u jdbc:hive2://localhost:10000
#show tables;
#select * from gdelt limit 10;

# Borramos el dataset de pruebas (hay que crearlo de nuevo)
#kite-dataset delete dataset:hive:gdelt

### END NOT RUN ###

#---------------------------------------------------------------------------------------------------------------
# Correr el agente de flume (desde playground/felipe)
# NOTA: Preferí usar un sink en Avro directo al HDFS en lugar de la cosa experimental con Kite
flume-ng agent -n GDELTAgent -Xmx300m --conf flume -f flume/gdelt-agent.conf

# Abrir otra conexión al contenedor (desde otra terminal)
docker exec -it hadoop-pseudo-proyecto /bin/zsh
su itam

# Correr el código que baja los datos y los mete al spoolDir (desde playground/felipe)
./process_background.sh data/raw data/spool 5

```


## Resultados

Analítica con HIVE

```{r, eval=FALSE}

############### Eventos de México entre eventos totales. ###############

select MonthYear, c_mx/c prop from (SELECT MonthYear, count(*) c, count(IF(ActionGeo_CountryCode = 'MX',1,NULL)) c_mx 
FROM gdelt WHERE MonthYear > 0 
GROUP BY MonthYear
ORDER BY MonthYear);

#### exportamos datos

hive -e "set hive.cli.print.header=true; select MonthYear, c_mx/c prop from (SELECT MonthYear, count(*) c, count(IF(ActionGeo_CountryCode = 'MX',1,NULL)) c_mx 
FROM gdelt WHERE MonthYear > 0 
GROUP BY MonthYear
ORDER BY MonthYear)" | sed 's/[\t]/,/g'  > /data/eventos.csv


####################### CONFLICTOS EN MEXICO ########################

SELECT MonthYear, ActionGeo_Long, ActionGeo_Lat, ActionGeo_FullName,
        count(*) c_mx
FROM gdelt
WHERE EventRootCode = '19' AND ActionGeo_CountryCode = 'MX'
GROUP BY MonthYear, ActionGeo_Long, ActionGeo_Lat, ActionGeo_FullName
ORDER BY MonthYear;



############### ACTORES  MEXICO VS EL MUNDO ##############################

SELECT Year, Actor1Name, Actor2Name, Count FROM (
    SELECT Actor1Name, Actor2Name, Year, COUNT(*) Count, 
    RANK() OVER(PARTITION BY YEAR ORDER BY Count DESC) rank
    FROM 
    (
        SELECT Actor1Name, Actor2Name,  Year 
        FROM gdelt
        WHERE Actor1Name < Actor2Name and Actor1CountryCode != '' and 
        Actor2CountryCode != '' and Actor1CountryCode!=Actor2CountryCode 
        and ActionGeo_CountryCode='MX'),
    (
        SELECT Actor2Name Actor1Name, Actor1Name Actor2Name, Year 
        FROM gdelt
        WHERE Actor1Name > Actor2Name  and Actor1CountryCode != ''
        and Actor2CountryCode != '' and Actor1CountryCode!= Actor2CountryCode
        and ActionGeo_CountryCode='MX'),
        WHERE Actor1Name IS NOT null
        AND Actor2Name IS NOT null
        GROUP EACH BY 1, 2, 3
        HAVING Count > 100
    )
    WHERE rank=1
    ORDER BY Year;

```


<center><img  src="/Users/usuario/Dropbox/Documents/ITAM/primavera2015/final-big-data/images/mapas.png" height="800px" width="620px" /></center>

<br></br>

<center><img  src="/Users/usuario/Dropbox/Documents/ITAM/primavera2015/final-big-data/images/mex.png" height="800px" width="620px" /></center>

<br></br>

| Year|Actor1Name |Actor2Name    | Count|
|----:|:----------|:-------------|-----:|
| 1980|MEXICO     |UNITED STATES |   118|
| 1981|MEXICO     |UNITED STATES |   179|
| 1982|MEXICO     |UNITED STATES |   225|
| 1983|MEXICO     |UNITED STATES |   326|
| 1984|MEXICO     |UNITED STATES |   291|
| 1985|MEXICO     |UNITED STATES |   433|
| 1986|MEXICO     |UNITED STATES |   334|
| 1987|MEXICO     |UNITED STATES |   224|
| 1988|MEXICO     |UNITED STATES |   267|
| 1989|MEXICO     |UNITED STATES |   213|






Analítica PIG

```{r , eval=FALSE}

### CARGAMOS AVRO

gdelt = LOAD 'datasets/gdelt' USING org.apache.pig.piggybank.storage.avro.AvroStorage();

### visualizando la tabla de gdelt

head = LIMIT gelt 5;
DUMP head; 


### visualizamos Actores distintos.

Actor1Name = DISTINCT (FOREACH gdelt GENERATE Actor1Name);
DUMP Actor1Name;


### Visualizamos los países

gdelt2 = FILTER gdelt by ActionGeo_CountryCode != '';
country = DISTINCT (FOREACH gdelt2 GENERATE ActionGeo_CountryCode);
DUMP country;


### Contamos los eventos por actor principal pais.

gdelt_grp = GROUP gdelt by Actor1CountryCode;
country = FOREACH gdelt GENERATE Actor1CountryCode;
gdelt_cnt = FOREACH gdelt_grp GENERATE GROUP AS country, COUNT(gdelt) as count;
DUMP gdelt_cnt;


### Contar el total de lineas
### SELECT COUNT(*) FROM gdelt;

gdelt_grp = GROUP gdelt all;
gdelt_cnt = FOREACH gdelt_grp GENERATE COUNT(gdelt);
DUMP gdelt_cnt;

### Contar los distitntos EventCode  (clasificacion del evento)
### SELECT COUNT(DISTINCT EventCode) FROM gdelt;

event = DISTINCT(FOREACH gdelt GENERATE EventCode);
event_grp = GROUP event all;
event_cnt = FOREACH event_grp GENERATE COUNT(event);
DUMP event_cnt;
```

Analítica pyspark


```{r, eval= FALSE}

import csv
from io import StringIO

def load_tsv(archivo):
    return csv.reader(StringIO(archivo[1]), delimiter='\t')


####  Actor1Code

gdelt = sc.textFile("hdfs://localhost/user/itam/data/20130412.export.CSV").flatMap(load_tsv)
gdelt.take(3)[5]

#### Contamos el numero diferente de eventos
#### gdelt = sc.textFile("hdfs://localhost/user/itam/data/20130412.export.CSV")

gdelt.map(lambda line: (line.split('\t')[22]))\
.distinct()\
.count()

### contamos registro
gdelt.count()

```

